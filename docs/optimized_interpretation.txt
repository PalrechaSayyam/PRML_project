Random Forest (RF_Optimized) - Interpretive Analysis
| Task              | Key Parameters                                                                                       | Interpretation                                                                                                                                                                                                 |
| ----------------- | ---------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **NR-AhR**        | `max_depth=10`, `min_samples_split=8`, `min_samples_leaf=1`, `max_features=sqrt`, `n_estimators=100` | Moderate tree depth with higher split threshold indicates moderate task complexity with non-linear patterns, but also slight overfitting risk control. Suggests AhR task is noisy with few reliable positives. |
| **NR-AR-LBD**     | `max_features=log2`, `min_samples_split=2`, `n_estimators=100`                                       | Use of log2 features means more randomness; lower split threshold allows detailed splits → task likely has correlated fingerprints and benefits from randomization for generalization.                         |
| **NR-Aromatase**  | `max_depth=20`, `min_samples_split=4`, `n_estimators=100`, `max_features=sqrt`                       | Deep trees imply strong nonlinear relationships; Aromatase task likely requires capturing subtle chemical substructure variations — higher complexity and well-defined patterns.                               |
| **NR-AR**         | `max_depth=10`, `min_samples_split=8`, `max_features=sqrt`                                           | Similar to NR-AhR; moderate complexity. Possibly has mixed substructure–activity patterns but not deeply hierarchical.                                                                                         |
| **NR-ER-LBD**     | `min_samples_split=3`, `min_samples_leaf=3`, `max_features=sqrt`                                     | Moderate regularization → task is noisy or small; higher minimum leaf count prevents overfitting. Likely fewer actives or unstable structure–activity trends.                                                  |
| **NR-ER**         | `max_depth=10`, `min_samples_split=4`, `min_samples_leaf=3`, `n_estimators=200`                      | Deeper model and more trees → task benefits from ensemble averaging; suggests complex and possibly imbalanced target with high variability in actives.                                                         |
| **NR-PPAR-gamma** | `min_samples_split=3`, `min_samples_leaf=3`, `max_features=sqrt`                                     | Very regularized configuration; task likely suffers from severe imbalance or noisy positives.                                                                                                                  |
| **SR-ARE**        | `min_samples_split=3`, `min_samples_leaf=3`, `max_features=sqrt`                                     | Similar to NR-PPAR-gamma — consistent signal but few reliable actives. The model avoids overfitting by forcing larger leaf sizes.                                                                              |
| **SR-ATAD5**      | `min_samples_split=3`, `min_samples_leaf=3`, `max_features=sqrt`                                     | Balanced configuration suggests weak or subtle feature–activity mapping. Likely data limited.                                                                                                                  |
| **SR-HSE**        | `min_samples_split=3`, `min_samples_leaf=5`, `max_features=log2`, `n_estimators=400`                 | High regularization and high ensemble size → very noisy task; model needs averaging to stabilize predictions. Likely few actives or overlapping chemical signatures.                                           |
| **SR-MMP**        | `min_samples_split=3`, `min_samples_leaf=3`, `max_features=sqrt`                                     | Similar to SR-ARE; low complexity, imbalanced.                                                                                                                                                                 |
| **SR-p53**        | `min_samples_split=3`, `min_samples_leaf=3`, `max_features=sqrt`                                     | Balanced and regularized → indicates that p53 assay may have a small, consistent active cluster but weak separability.                                                                                         |

RF Summary Insight:
Deep trees (Aromatase) → complex nonlinear substructure–activity mapping.
Shallow/moderate trees with high leaf size (SR tasks) → noisy or imbalanced data; model relies on ensemble averaging, not deep feature hierarchies.
Log2 features (AR-LBD, HSE) → correlated fingerprints, requiring more randomness to prevent overfitting.
Interpretation:
RF optimization suggests that nuclear receptor (NR) tasks often require moderate-to-deep models (non-linear), while stress response (SR) tasks favor regularized, 
shallower configurations — a direct reflection of their biological and chemical variability.


XGBoost (XGB_Optimized) — Interpretive Analysis
| Task              | Key Parameters                                                                                       | Interpretation                                                                                                                                                                         |
| ----------------- | ---------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **NR-AhR**        | `max_depth=4`, `learning_rate=0.0467`, `subsample=0.84`, `colsample_bytree=0.62`, `n_estimators=414` | Very low learning rate + high estimators → slow, controlled learning. Indicates noise or subtle patterns. AhR task benefits from gradual boosting, implying dispersed active clusters. |
| **NR-AR-LBD**     | `max_depth=3`, `learning_rate=0.134`, `subsample=0.84`, `colsample_bytree=0.88`, `n_estimators=162`  | Shallow trees → low complexity task; clear signal separability.                                                                                                                        |
| **NR-Aromatase**  | Same as AR-LBD                                                                                       | Indicates similar simplicity; suggests Aromatase task linearly separable after feature expansion.                                                                                      |
| **NR-AR**         | `max_depth=4`, `learning_rate=0.012`, `subsample=0.98`, `colsample_bytree=0.99`, `n_estimators=343`  | Extremely low learning rate + large ensemble → highly non-linear, subtle decision boundaries. Strong regularization indicates challenging separability.                                |
| **NR-ER-LBD**     | `max_depth=8`, `learning_rate=0.121`, `subsample=0.68`, `colsample_bytree=0.67`, `n_estimators=108`  | Deeper trees but fewer rounds → complex but structured signal. Suggests task has well-defined but intricate relationships.                                                             |
| **NR-ER**         | `max_depth=4`, `learning_rate=0.048`, `subsample=0.77`, `colsample_bytree=0.72`, `n_estimators=173`  | Moderate configuration; balanced between under- and overfitting. Indicates mid-level complexity.                                                                                       |
| **NR-PPAR-gamma** | `max_depth=8`, `learning_rate=0.09`, `subsample=0.84`, `colsample_bytree=0.66`, `n_estimators=250`   | Deep but well-regularized; subtle, nonlinear relationships but decent separability.                                                                                                    |
| **SR-ARE**        | Same as PPAR-gamma                                                                                   | Similar interpretation — moderate to high complexity but stable pattern recognition.                                                                                                   |
| **SR-ATAD5**      | `max_depth=8`, `learning_rate=0.121`, `subsample=0.68`, `colsample_bytree=0.67`, `n_estimators=108`  | Deep but few estimators → task benefits from strong learners; likely small data.                                                                                                       |
| **SR-HSE**        | Same as SR-ATAD5                                                                                     | Suggests similar characteristics — noisy but moderately nonlinear.                                                                                                                     |
| **SR-MMP**        | Same as PPAR-gamma                                                                                   | Deep trees but controlled learning rate → robust non-linear learning with regularization.                                                                                              |
| **SR-p53**        | Same as SR-ATAD5                                                                                     | Moderate complexity; suggests mid-size data, subtle patterns.                                                                                                                          |

XGB Summary Insight:
Low learning rate + many estimators (NR-AR, NR-AhR) → noisy, high-variance tasks needing cautious, incremental fitting.
Shallow trees (AR-LBD, Aromatase) → easily separable, stable structure–activity mappings.
Deep trees + moderate learning rate (PPAR-γ, SR tasks) → nonlinear but consistent signals; complex but learnable patterns.
Interpretation:
XGBoost learns gradually for complex NR tasks and uses deeper trees for SR tasks, showing that noise regularization and structure–activity depth vary strongly by target.

Cross-Model Insights
| Observation                                                                                 | Interpretation                                                                                                                 |
| ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| **RF max_depth moderate (10), XGB low learning rate (0.01–0.05)** in NR tasks (AR, AhR, ER) | Both models regularized — these are hard, noisy targets with subtle non-linearities.                                           |
| **RF shallow but many leaves (SR tasks)**                                                   | SR tasks show less clear signal, requiring stronger bias control — small, imbalanced datasets.                                 |
| **XGB deeper trees for SR tasks**                                                           | Gradient boosting captured subtle, weak patterns missed by RF — GBDT’s sequential refinement handles difficult regions better. |
Common trend: deeper trees or slower learning → difficult or high-noise tasks.                                                                                                                                       
